{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import torch\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env._max_episode_steps = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# NN Q-learning implementation #\n",
    "################################\n",
    "\n",
    "env.reset()\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "nn_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4,32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32,32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32,2),\n",
    ")\n",
    "\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.95\n",
    "\n",
    "exploration_rate = 1\n",
    "memory = deque(maxlen=1000000)\n",
    "\n",
    "nn_model_optimizer = torch.optim.Adam(nn_model.parameters(), lr=0.001)\n",
    "\n",
    "def select_action(state):\n",
    "    global exploration_rate\n",
    "    \n",
    "    if np.random.rand() < exploration_rate:\n",
    "        return action_space.sample()\n",
    "    state = torch.from_numpy(state.astype(\"float32\")) \n",
    "    q_values = nn_model(state)\n",
    "    return torch.argmax(q_values).item()\n",
    "\n",
    "def remember(last_state,action,reward,next_state,done):\n",
    "    global memory\n",
    "    memory.append((last_state,action,reward,next_state,done))\n",
    "\n",
    "def experience_replay():\n",
    "    global memory,EXPLORATION_MIN,EXPLORATION_DECAY,BATCH_SIZE,GAMMA,exploration_rate\n",
    "\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    batch = random.sample(memory,BATCH_SIZE)\n",
    "    \n",
    "    for last_state, action, reward, next_state, done in batch:\n",
    "        q_update = reward\n",
    "        if not done:\n",
    "            q_update = (reward + GAMMA * torch.amax(nn_model(torch.from_numpy(next_state.astype(\"float32\")))).item())\n",
    "        q_values = nn_model(torch.from_numpy(last_state.astype(\"float32\")))\n",
    "        q_values[action] = q_update\n",
    "        \n",
    "        #fit\n",
    "        l = loss(nn_model(torch.from_numpy(last_state.astype(\"float32\"))), q_values)\n",
    "        nn_model.zero_grad()\n",
    "        l.backward()\n",
    "        nn_model_optimizer.step()\n",
    "    \n",
    "    if exploration_rate > EXPLORATION_MIN:\n",
    "        exploration_rate *= EXPLORATION_DECAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 33 steps with 31.0 total reward\n",
      "Episode 1 finished after 34 steps with 32.0 total reward\n",
      "Episode 2 finished after 15 steps with 13.0 total reward\n",
      "Episode 3 finished after 13 steps with 11.0 total reward\n",
      "Episode 4 finished after 8 steps with 6.0 total reward\n",
      "Episode 5 finished after 10 steps with 8.0 total reward\n",
      "Episode 6 finished after 25 steps with 23.0 total reward\n",
      "Episode 7 finished after 15 steps with 13.0 total reward\n",
      "Episode 8 finished after 15 steps with 13.0 total reward\n",
      "Episode 9 finished after 11 steps with 9.0 total reward\n",
      "Episode 10 finished after 14 steps with 12.0 total reward\n",
      "Episode 11 finished after 11 steps with 9.0 total reward\n",
      "Episode 12 finished after 13 steps with 11.0 total reward\n",
      "Episode 13 finished after 15 steps with 13.0 total reward\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-91bb005889e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-0a6021a92372>\u001b[0m in \u001b[0;36mexperience_replay\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[0mnn_model_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\comp432\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\comp432\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "episodes = 300\n",
    "episodes_steps = np.array([])\n",
    "num_solved = 0\n",
    "\n",
    "for episode_i in range(episodes):\n",
    "    last_state = env.reset()\n",
    "    steps = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        env.render()\n",
    "        steps += 1\n",
    "        action = select_action(last_state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        reward = reward if not done else -reward\n",
    "        remember(last_state,action,reward,next_state,done)\n",
    "        experience_replay()\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode {} finished after {} steps with {} total reward\".format(episode_i,steps,total_reward))\n",
    "            if total_reward > 250:\n",
    "                num_solved += 1\n",
    "            break\n",
    "    \n",
    "        last_state = next_state\n",
    "    \n",
    "    episodes_steps = np.append(episodes_steps,steps)\n",
    "    \n",
    "    if num_solved > 10:\n",
    "        break\n",
    "        \n",
    "env.close()\n",
    "\n",
    "plt.plot(episodes_steps)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
